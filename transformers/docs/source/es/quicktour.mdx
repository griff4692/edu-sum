<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# Quick tour

[[open-in-colab]]

¬°Entra en marcha con los ü§ó Transformers! Comienza usando [`pipeline`] para una inferencia veloz, carga un modelo preentrenado y un tokenizador con una [AutoClass](./model_doc/auto) para resolver tu tarea de texto, visi√≥n o audio.

<Tip>

Todos los ejemplos de c√≥digo presentados en la documentaci√≥n tienen un bot√≥n arriba a la izquierda para elegir entre Pytorch y TensorFlow.
Si no fuese as√≠, se espera que el c√≥digo funcione para ambos backends sin ning√∫n cambio.

</Tip>

## Pipeline

[`pipeline`] es la forma m√°s f√°cil de usar un modelo preentrenado para una tarea dada.

<Youtube id="tiZFewofSLM"/>

El [`pipeline`] soporta muchas tareas comunes listas para usar:

**Texto**:
* An√°lisis de Sentimientos: clasifica la polaridad de un texto dado.
* Generaci√≥n de texto (solo en ingl√©s): genera texto a partir de un input dado.
* Name entity recognition (NER): etiqueta cada palabra con la entidad que representa (persona, fecha, ubicaci√≥n, etc.).
* Responder preguntas: extrae la respuesta del contexto dado un contexto y una pregunta.
* Fill-mask: rellena el espacio faltante dado un texto con palabras enmascaradas.
* Summarization: genera un resumen de una secuencia larga de texto o un documento.
* Traducci√≥n: traduce un texto a otro idioma.
* Extracci√≥n de caracter√≠sticas: crea una representaci√≥n tensorial del texto.

**Imagen**:
* Clasificaci√≥n de im√°genes: clasifica una imagen.
* Segmentaci√≥n de im√°genes: clasifica cada pixel de una imagen.
* Detecci√≥n de objetos: detecta objetos dentro de una imagen.

**Audio**:
* Clasificaci√≥n de audios: asigna una etiqueta a un segmento de audio.
* Automatic speech recognition (ASR): transcribe datos de audio a un texto.

<Tip>

Para m√°s detalles acerca del [`pipeline`] y tareas asociadas, consulta la documentaci√≥n [aqu√≠](./main_classes/pipelines).

</Tip>

### Uso del Pipeline

En el siguiente ejemplo, usar√°s el [`pipeline`] para an√°lisis de sentimiento.

Instala las siguientes dependencias si a√∫n no lo has hecho:

<frameworkcontent>
<pt>
```bash
pip install torch
```
</pt>
<tf>
```bash
pip install tensorflow
```
</tf>
</frameworkcontent>

Importa [`pipeline`] y especifica la tarea que deseas completar:

```py
>>> from transformers import pipeline

>>> classifier = pipeline("sentiment-analysis")
```

El pipeline descarga y almacena en cach√© un [modelo preentrenado](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) por defecto y tokeniza para an√°lisis de sentimiento. Ahora puedes usar `classifier` en tu texto objetivo:

```py
>>> classifier("We are very happy to show you the ü§ó Transformers library.")
[{'label': 'POSITIVE', 'score': 0.9998}]
```

Para m√°s de un enunciado entrega una lista de frases al [`pipeline`] que devolver√° una lista de diccionarios:

```py
>>> results = classifier(["We are very happy to show you the ü§ó Transformers library.", "We hope you don't hate it."])
>>> for result in results:
...     print(f"label: {result['label']}, with score: {round(result['score'], 4)}")
label: POSITIVE, with score: 0.9998
label: NEGATIVE, with score: 0.5309
```

El [`pipeline`] tambi√©n puede iterar sobre un dataset entero. Comienza instalando la biblioteca [ü§ó Datasets](https://huggingface.co/docs/datasets/):

```bash
pip install datasets
```

Crea un [`pipeline`] con la tarea que deseas resolver y el modelo que quieres usar. Coloca el par√°metro `device` a `0` para poner los tensores en un dispositivo CUDA:

```py
>>> import torch
>>> from transformers import pipeline

>>> speech_recognizer = pipeline("automatic-speech-recognition", model="facebook/wav2vec2-base-960h", device=0)
```

A continuaci√≥n, carga el dataset (ve ü§ó Datasets [Quick Start](https://huggingface.co/docs/datasets/quickstart.html) para m√°s detalles) sobre el que quisieras iterar. Por ejemplo, vamos a cargar el dataset [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14):

```py
>>> from datasets import load_dataset, Audio

>>> dataset = load_dataset("PolyAI/minds14", name="en-US", split="train")  # doctest: +IGNORE_RESULT
```

Debemos asegurarnos de que la frecuencia de muestreo del conjunto de datos coincide con la frecuencia de muestreo con la que se entren√≥ `facebook/wav2vec2-base-960h`.

```py
>>> dataset = dataset.cast_column("audio", Audio(sampling_rate=speech_recognizer.feature_extractor.sampling_rate))
```

Los archivos de audio se cargan y remuestrean autom√°ticamente cuando se llama a la columna `"audio"`.
Extraigamos las matrices de forma de onda cruda de las primeras 4 muestras y pas√©mosla como una lista al pipeline:

```py
>>> result = speech_recognizer(dataset[:4]["audio"])
>>> print([d["text"] for d in result])
['I WOULD LIKE TO SET UP A JOINT ACCOUNT WITH MY PARTNER HOW DO I PROCEED WITH DOING THAT', "FONDERING HOW I'D SET UP A JOIN TO HET WITH MY WIFE AND WHERE THE AP MIGHT BE", "I I'D LIKE TOY SET UP A JOINT ACCOUNT WITH MY PARTNER I'M NOT SEEING THE OPTION TO DO IT ON THE APSO I CALLED IN TO GET SOME HELP CAN I JUST DO IT OVER THE PHONE WITH YOU AND GIVE YOU THE INFORMATION OR SHOULD I DO IT IN THE AP AND I'M MISSING SOMETHING UQUETTE HAD PREFERRED TO JUST DO IT OVER THE PHONE OF POSSIBLE THINGS", 'HOW DO I TURN A JOIN A COUNT']
```

Para un dataset m√°s grande, donde los inputs son de mayor tama√±o (como en habla/audio o visi√≥n), querr√°s pasar un generador en lugar de una lista que carga todos los inputs en memoria. Ve la [documentaci√≥n del pipeline](./main_classes/pipelines) para m√°s informaci√≥n.

### Use otro modelo y otro tokenizador en el pipeline

El [`pipeline`] puede adaptarse a cualquier modelo del [Model Hub](https://huggingface.co/models) haciendo m√°s f√°cil adaptar el [`pipeline`] para otros casos de uso. Por ejemplo, si quisieras un modelo capaz de manejar texto en franc√©s, usa los tags en el Model Hub para filtrar entre los modelos apropiados. El resultado mejor filtrado devuelve un [modelo BERT](https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment) multilingual fine-tuned para el an√°lisis de sentimiento. Genial, ¬°vamos a usar este modelo!

```py
>>> model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
```

<frameworkcontent>
<pt>
Usa [`AutoModelForSequenceClassification`] y ['AutoTokenizer'] para cargar un modelo preentrenado y un tokenizador asociado (m√°s en un `AutoClass` debajo):

```py
>>> from transformers import AutoTokenizer, AutoModelForSequenceClassification

>>> model = AutoModelForSequenceClassification.from_pretrained(model_name)
>>> tokenizer = AutoTokenizer.from_pretrained(model_name)
```

</pt>

<tf>
Usa [`TFAutoModelForSequenceClassification`] y ['AutoTokenizer'] para cargar un modelo preentrenado y un tokenizador asociado (m√°s en un `TFAutoClass` debajo):

```py
>>> from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

>>> model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
>>> tokenizer = AutoTokenizer.from_pretrained(model_name)
```

</tf>
</frameworkcontent>

Despu√©s puedes especificar el modelo y el tokenizador en el [`pipeline`], y aplicar el `classifier` en tu texto objetivo:

```py
>>> classifier = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)
>>> classifier("Nous sommes tr√®s heureux de vous pr√©senter la biblioth√®que ü§ó Transformers.")
[{'label': '5 stars', 'score': 0.7273}]
```

Si no pudieras encontrar el modelo para tu caso respectivo de uso necesitar√°s ajustar un modelo preentrenado a tus datos. Mira nuestro [tutorial de fine-tuning](./training) para aprender c√≥mo. Finalmente, despu√©s de que has ajustado tu modelo preentrenado, ¬°por favor considera compartirlo (ve el tutorial [aqu√≠](./model_sharing)) con la comunidad en el Model Hub para democratizar el NLP! ü§ó

## AutoClass

<Youtube id="AhChOFRegn4"/>

Debajo del cap√≥, las clases [`AutoModelForSequenceClassification`] y [`AutoTokenizer`] trabajan juntas para dar poder al [`pipeline`]. Una [AutoClass](./model_doc/auto) es un atajo que autom√°ticamente recupera la arquitectura de un modelo preentrenado con su nombre o el path. S√≥lo necesitar√°s seleccionar el `AutoClass` apropiado para tu tarea y tu tokenizador asociado con [`AutoTokenizer`].

Regresemos a nuestro ejemplo y veamos c√≥mo puedes usar el `AutoClass` para reproducir los resultados del [`pipeline`].

### AutoTokenizer

Un tokenizador es responsable de procesar el texto a un formato que sea entendible para el modelo. Primero, el tokenizador separar√° el texto en palabras llamadas *tokens*. Hay m√∫ltiples reglas que gobiernan el proceso de tokenizaci√≥n incluyendo el c√≥mo separar una palabra y en qu√© nivel (aprende m√°s sobre tokenizaci√≥n [aqu√≠](./tokenizer_summary)). Lo m√°s importante es recordar que necesitar√°s instanciar el tokenizador con el mismo nombre del modelo para asegurar que est√°s usando las mismas reglas de tokenizaci√≥n con las que el modelo fue preentrenado.

Carga un tokenizador con [`AutoTokenizer`]:

```py
>>> from transformers import AutoTokenizer

>>> model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
>>> tokenizer = AutoTokenizer.from_pretrained(model_name)
```

Despu√©s, el tokenizador convierte los tokens a n√∫meros para construir un tensor que servir√° como input para el modelo. Esto es conocido como el *vocabulario* del modelo.

Pasa tu texto al tokenizador:

```py
>>> encoding = tokenizer("We are very happy to show you the ü§ó Transformers library.")
>>> print(encoding)
{'input_ids': [101, 11312, 10320, 12495, 19308, 10114, 11391, 10855, 10103, 100, 58263, 13299, 119, 102],
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

El tokenizador devolver√° un diccionario conteniendo:

* [input_ids](./glossary#input-ids): representaciones num√©ricas de los tokens.
* [atttention_mask](.glossary#attention-mask): indica cu√°les tokens deben ser atendidos.

Como con el [`pipeline`], el tokenizador aceptar√° una lista de inputs. Adem√°s, el tokenizador tambi√©n puede rellenar (pad, en ingl√©s) y truncar el texto para devolver un lote (batch, en ingl√©s) de longitud uniforme:

<frameworkcontent>
<pt>
```py
>>> pt_batch = tokenizer(
...     ["We are very happy to show you the ü§ó Transformers library.", "We hope you don't hate it."],
...     padding=True,
...     truncation=True,
...     max_length=512,
...     return_tensors="pt",
... )
```
</pt>
<tf>
```py
>>> tf_batch = tokenizer(
...     ["We are very happy to show you the ü§ó Transformers library.", "We hope you don't hate it."],
...     padding=True,
...     truncation=True,
...     max_length=512,
...     return_tensors="tf",
... )
```
</tf>
</frameworkcontent>

Lee el tutorial de [preprocessing](./preprocessing) para m√°s detalles acerca de la tokenizaci√≥n.

### AutoModel

<frameworkcontent>
<pt>
ü§ó Transformers provee una forma simple y unificada de cargar tus instancias preentrenadas. Esto significa que puedes cargar un [`AutoModel`] como cargar√≠as un [`AutoTokenizer`]. La √∫nica diferencia es seleccionar el [`AutoModel`] correcto para la tarea. Ya que est√°s clasificando texto, o secuencias, carga [`AutoModelForSequenceClassification`]:

```py
>>> from transformers import AutoModelForSequenceClassification

>>> model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
>>> pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)
```

<Tip>

Ve el [task summary](./task_summary) para revisar qu√© clase del [`AutoModel`] deber√≠as usar para cada tarea.

</Tip>

Ahora puedes pasar tu lote (batch) preprocesado de inputs directamente al modelo. Solo tienes que desempacar el diccionario a√±adiendo `**`:

```py
>>> pt_outputs = pt_model(**pt_batch)
```

El modelo producir√° las activaciones finales en el atributo `logits`. Aplica la funci√≥n softmax a `logits` para obtener las probabilidades:

```py
>>> from torch import nn

>>> pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)
>>> print(pt_predictions)
tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],
        [0.2084, 0.1826, 0.1969, 0.1755, 0.2365]], grad_fn=<SoftmaxBackward0>)
```
</pt>
<tf>
ü§ó Transformers provee una forma simple y unificada de cargar tus instancias preentrenadas. Esto significa que puedes cargar un [`TFAutoModel`] como cargar√≠as un [`AutoTokenizer`]. La √∫nica diferencia es seleccionar el [`TFAutoModel`] correcto para la tarea. Ya que est√°s clasificando texto, o secuencias, carga [`TFAutoModelForSequenceClassification`]:

```py
>>> from transformers import TFAutoModelForSequenceClassification

>>> model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
>>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
```

<Tip>
  Ve el [task summary](./task_summary) para revisar qu√© clase del [`AutoModel`]
  deber√≠as usar para cada tarea.
</Tip>

Ahora puedes pasar tu lote preprocesado de inputs directamente al modelo pasando las llaves del diccionario directamente a los tensores:

```py
>>> tf_outputs = tf_model(tf_batch)
```

El modelo producir√° las activaciones finales en el atributo `logits`. Aplica la funci√≥n softmax a `logits` para obtener las probabilidades:

```py
>>> import tensorflow as tf

>>> tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)
>>> print(tf.math.round(tf_predictions * 10**4) / 10**4)
tf.Tensor(
[[0.0021 0.0018 0.0116 0.2121 0.7725]
 [0.2084 0.1826 0.1969 0.1755  0.2365]], shape=(2, 5), dtype=float32)
```
</tf>
</frameworkcontent>

<Tip>

Todos los modelos de ü§ó Transformers (PyTorch o TensorFlow) producir√°n los tensores *antes* de la funci√≥n de activaci√≥n
final (como softmax) porque la funci√≥n de activaci√≥n final es com√∫nmente fusionada con la p√©rdida.

</Tip>

Los modelos son [`torch.nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) o [`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model) est√°ndares as√≠ que podr√°s usarlos en tu training loop usual. Sin embargo, para facilitar las cosas, ü§ó Transformers provee una clase [`Trainer`] para PyTorch que a√±ade funcionalidades para entrenamiento distribuido, precici√≥n mixta, y m√°s. Para TensorFlow, puedes usar el m√©todo `fit` desde [Keras](https://keras.io/). Consulta el [tutorial de entrenamiento](./training) para m√°s detalles.

<Tip>

Los outputs del modelo de ü§ó Transformers son dataclasses especiales por lo que sus atributos pueden ser completados en un IDE.
Los outputs del modelo tambi√©n se comportan como tuplas o diccionarios (e.g., puedes indexar con un entero, un slice o una cadena) en cuyo caso los atributos que son `None` son ignorados.

</Tip>

### Guarda un modelo

<frameworkcontent>
<pt>
Una vez que tu modelo est√© fine-tuned puedes guardarlo con tu tokenizador usando [`PreTrainedModel.save_pretrained`]:

```py
>>> pt_save_directory = "./pt_save_pretrained"
>>> tokenizer.save_pretrained(pt_save_directory)  # doctest: +IGNORE_RESULT
>>> pt_model.save_pretrained(pt_save_directory)
```

Cuando quieras usar el modelo otra vez c√°rgalo con [`PreTrainedModel.from_pretrained`]:

```py
>>> pt_model = AutoModelForSequenceClassification.from_pretrained("./pt_save_pretrained")
```

</pt>

<tf>
Una vez que tu modelo est√© fine-tuned puedes guardarlo con tu tokenizador usando [`TFPreTrainedModel.save_pretrained`]:

```py
>>> tf_save_directory = "./tf_save_pretrained"
>>> tokenizer.save_pretrained(tf_save_directory)  # doctest: +IGNORE_RESULT
>>> tf_model.save_pretrained(tf_save_directory)
```

Cuando quieras usar el modelo otra vez c√°rgalo con [`TFPreTrainedModel.from_pretrained`]:

```py
>>> tf_model = TFAutoModelForSequenceClassification.from_pretrained("./tf_save_pretrained")
```
</tf>
</frameworkcontent>

Una caracter√≠stica particularmente cool de ü§ó Transformers es la habilidad de guardar el modelo y cargarlo como un modelo de PyTorch o TensorFlow. El par√°metro `from_pt` o `from_tf` puede convertir el modelo de un framework al otro:

<frameworkcontent>
<pt>
```py
>>> from transformers import AutoModel

>>> tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)
>>> pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)
```
</pt>
<tf>
```py
>>> from transformers import TFAutoModel

>>> tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)
>>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)
```
</tf>
</frameworkcontent>
